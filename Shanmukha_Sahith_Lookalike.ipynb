{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XNYyHlkhbii",
        "outputId": "80fb0798-b6ef-4ce2-831f-ae860c528364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasketch in /usr/local/lib/python3.11/dist-packages (1.6.5)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.11/dist-packages (from datasketch) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasketch) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasketch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dataset"
      ],
      "metadata": {
        "id": "M_mf_xCG5RQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers = pd.read_csv('Customers.csv')\n",
        "products = pd.read_csv('Products.csv')\n",
        "transactions = pd.read_csv('Transactions.csv')\n",
        "\n",
        "print(customers.info())\n",
        "print(products.info())\n",
        "print(transactions.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRRcNRbShpvW",
        "outputId": "d0326dab-f2ba-4385-d9d5-5f04576c9184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200 entries, 0 to 199\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   CustomerID    200 non-null    object\n",
            " 1   CustomerName  200 non-null    object\n",
            " 2   Region        200 non-null    object\n",
            " 3   SignupDate    200 non-null    object\n",
            "dtypes: object(4)\n",
            "memory usage: 6.4+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 4 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   ProductID    100 non-null    object \n",
            " 1   ProductName  100 non-null    object \n",
            " 2   Category     100 non-null    object \n",
            " 3   Price        100 non-null    float64\n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 3.3+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 7 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   TransactionID    1000 non-null   object \n",
            " 1   CustomerID       1000 non-null   object \n",
            " 2   ProductID        1000 non-null   object \n",
            " 3   TransactionDate  1000 non-null   object \n",
            " 4   Quantity         1000 non-null   int64  \n",
            " 5   TotalValue       1000 non-null   float64\n",
            " 6   Price            1000 non-null   float64\n",
            "dtypes: float64(2), int64(1), object(4)\n",
            "memory usage: 54.8+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging the datasets"
      ],
      "metadata": {
        "id": "LsOQYrp77V5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_customers = pd.merge(transactions, customers, on='CustomerID', how='inner')\n",
        "\n",
        "data = pd.merge(transactions_customers, products, on='ProductID', how='inner')\n",
        "\n",
        "print(data.info())\n",
        "data.to_csv(\"final_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8rdvCVIhwTn",
        "outputId": "00f5f62e-a04e-4a45-bc04-4f9cb097feb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 13 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   TransactionID    1000 non-null   object \n",
            " 1   CustomerID       1000 non-null   object \n",
            " 2   ProductID        1000 non-null   object \n",
            " 3   TransactionDate  1000 non-null   object \n",
            " 4   Quantity         1000 non-null   int64  \n",
            " 5   TotalValue       1000 non-null   float64\n",
            " 6   Price_x          1000 non-null   float64\n",
            " 7   CustomerName     1000 non-null   object \n",
            " 8   Region           1000 non-null   object \n",
            " 9   SignupDate       1000 non-null   object \n",
            " 10  ProductName      1000 non-null   object \n",
            " 11  Category         1000 non-null   object \n",
            " 12  Price_y          1000 non-null   float64\n",
            "dtypes: float64(3), int64(1), object(9)\n",
            "memory usage: 101.7+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = pd.read_csv(\"/content/final_data.csv\")\n",
        "final_data.info()\n",
        "final_data = final_data.loc[:, ~final_data.columns.str.contains('^Unnamed')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5ITCpD1h2pf",
        "outputId": "c8a0c233-95eb-40ed-925b-94e0a20d8a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 13 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Unnamed: 0       1000 non-null   int64  \n",
            " 1   CustomerID       1000 non-null   object \n",
            " 2   CustomerName     1000 non-null   object \n",
            " 3   Region           1000 non-null   object \n",
            " 4   SignupDate       1000 non-null   object \n",
            " 5   TransactionID    1000 non-null   object \n",
            " 6   ProductID        1000 non-null   object \n",
            " 7   TransactionDate  1000 non-null   object \n",
            " 8   Quantity         1000 non-null   int64  \n",
            " 9   TotalValue       1000 non-null   float64\n",
            " 10  Price            1000 non-null   float64\n",
            " 11  ProductName      1000 non-null   object \n",
            " 12  Category         1000 non-null   object \n",
            "dtypes: float64(2), int64(2), object(9)\n",
            "memory usage: 101.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping the data based on unique customers"
      ],
      "metadata": {
        "id": "cQSvXpKP7ZyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = final_data.groupby(\n",
        "    ['CustomerID', 'CustomerName', 'Region', 'SignupDate'], as_index=False\n",
        ").agg({\n",
        "    'TransactionID': list,\n",
        "    'ProductID': list,\n",
        "    'TransactionDate': list,\n",
        "    'Quantity': list,\n",
        "    'TotalValue': list,\n",
        "    'Price': list,\n",
        "    'ProductName': list,\n",
        "    'Category': list\n",
        "})\n",
        "merged_data.to_csv(\"merged_data.csv\")"
      ],
      "metadata": {
        "id": "lJhr0Ty0iUwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Select the first 20 customer IDs\n",
        "sampled_customer_ids = merged_data['CustomerID'].iloc[:20]\n",
        "\n",
        "# Filter the data to only include the sampled customer IDs\n",
        "sampled_data = merged_data[merged_data['CustomerID'].isin(sampled_customer_ids)]\n",
        "\n",
        "# Create a DataFrame with the customer IDs\n",
        "customer_ids_df = sampled_data[['CustomerID']]\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "customer_ids_df.to_csv('/content/data/seed.csv', index=False)\n"
      ],
      "metadata": {
        "id": "UCvHYGKViYA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BCGhedTH7g-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using LSH to find similar users\n",
        "\n",
        "Locality-Sensitive Hashing (LSH) is an efficient algorithm for finding similar items in large datasets by hashing items into buckets such that similar items have a high probability of colliding in the same bucket. Using the Jaccard similarity metric, LSH can efficiently compare sets (e.g., user behavior or preferences) by focusing on overlapping elements rather than comparing all pairwise combinations.\n",
        "\n",
        "Metric:\n",
        "Jaccard Similarity representing users' attributes or behaviors."
      ],
      "metadata": {
        "id": "t80acaST7hR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "from yaml import CLoader as Loader, load\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function for reading a YAML configuration file\n",
        "def read_config(path):\n",
        "    \"\"\"\n",
        "    Read a YAML configuration file and return it as a dictionary.\n",
        "\n",
        "    :param path: Path to the YAML configuration file\n",
        "    :return: Dictionary containing the configuration settings\n",
        "    \"\"\"\n",
        "    with open(path) as stream:\n",
        "        config = load(stream, Loader=Loader)\n",
        "    return config\n",
        "\n",
        "# Calculate feature importance (i.e. ranking the users) based on probabilities\n",
        "def feat_imp(p, q):\n",
        "    \"\"\"\n",
        "    Calculate feature importance given seed set probability of a feature and global\n",
        "    probability for the same feature.\n",
        "\n",
        "    :param p: Probability in the seed set\n",
        "    :param q: Global probability\n",
        "    :return: Feature importance score\n",
        "    \"\"\"\n",
        "    return (p - q) * np.log((p * (1 - q)) / ((1 - p) * q))\n",
        "\n",
        "# Calculate the counts of features in the dataset\n",
        "def count_fn(data, features, list_cols):\n",
        "    \"\"\"\n",
        "    Calculate the count of features in the dataset and their probabilities.\n",
        "\n",
        "    :param data: DataFrame containing user records\n",
        "    :param features: List of columns in the dataset\n",
        "    :param list_cols: Columns that can have multiple values per user\n",
        "    :return: DataFrame with feature count values and probabilities\n",
        "    \"\"\"\n",
        "    count_df = pd.DataFrame(columns=[\"value\", \"count\", \"feature\"])\n",
        "    for col in features:\n",
        "        if col not in list_cols:\n",
        "            counts = data[col].value_counts()\n",
        "        else:\n",
        "            counts = pd.Series(Counter(chain.from_iterable(x for x in data[col])))\n",
        "        counts = counts.reset_index()\n",
        "        counts.columns = [\"value\", \"count\"]\n",
        "        counts[\"feature\"] = col\n",
        "        count_df = pd.concat([count_df, counts])\n",
        "    count_df[\"sum\"] = count_df.groupby(\"feature\")[\"count\"].transform(sum)\n",
        "    count_df[\"prob\"] = count_df[\"count\"] / count_df[\"sum\"]\n",
        "    count_df = count_df[[\"feature\", \"value\", \"prob\"]]\n",
        "    return count_df\n",
        "\n",
        "# Convert column values into strings with column name as a prefix\n",
        "def conv_values(v, c, list_c):\n",
        "    \"\"\"\n",
        "    Convert column values into strings with the column name as a prefix.\n",
        "\n",
        "    :param v: Feature value\n",
        "    :param c: Column name\n",
        "    :param list_c: Boolean, True if c is a list column, False otherwise\n",
        "    :return: List of string-prefixed feature values or a single string\n",
        "    \"\"\"\n",
        "    if list_c:\n",
        "        final_v = []\n",
        "        for v_ in v:\n",
        "            final_v.append(f\"{c}_{str(v_)}\")\n",
        "    else:\n",
        "        final_v = f\"{c}_{str(v)}\"\n",
        "    return final_v\n",
        "\n",
        "def flatten_list(f):\n",
        "    \"\"\"\n",
        "    Flatten a list of lists.\n",
        "\n",
        "    :param f: List of lists\n",
        "    :return: A flat list\n",
        "    \"\"\"\n",
        "    f_l = []\n",
        "    for f_ in f:\n",
        "        if isinstance(f_, list):\n",
        "            f_l.extend(f_)\n",
        "        else:\n",
        "            f_l.append(f_)\n",
        "    return f_l\n"
      ],
      "metadata": {
        "id": "ty8UNyoMir6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def score_fn(data, count_path, features, list_cols, seed_ids, neighbors, label):\n",
        "    \"\"\"\n",
        "    Calculate a score for each user in the extended set based on feature importance.\n",
        "\n",
        "    :param data: DataFrame containing user features\n",
        "    :param count_path: Path to the feature count file\n",
        "    :param features: Features in the dataset\n",
        "    :param list_cols: Columns that can have multiple values per user\n",
        "    :param seed_ids: Customer IDs part of the seed set\n",
        "    :param neighbors: Customer IDs extracted from the LSH graph\n",
        "    :param label: Label column indicating whether the user clicked the ad or not\n",
        "    :return: DataFrame containing user features along with a score for each user\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the feature count file\n",
        "    count_df = pd.read_csv(count_path)\n",
        "    seed_df = data[data[\"CustomerID\"].isin(seed_ids)]\n",
        "    seed_count = count_fn(seed_df, features, list_cols)\n",
        "    seed_count.rename({\"prob\": \"s_prob\"}, axis=1, inplace=True)\n",
        "    seed_count = seed_count.merge(count_df, on=[\"feature\", \"value\"], how=\"left\")\n",
        "    seed_count[\"imp\"] = seed_count.apply(lambda x: feat_imp(x[\"s_prob\"], x[\"prob\"]), axis=1)\n",
        "    seed_count[\"feat\"] = seed_count[\"feature\"] + \"_\" + seed_count[\"value\"].astype(str)\n",
        "    seed_count = seed_count[[\"feat\", \"imp\"]]\n",
        "    df = data.drop(label, axis=1)\n",
        "    df = df[df[\"id\"].isin(neighbors)]\n",
        "    for f in features:\n",
        "        list_c = f in list_cols\n",
        "        df[f] = df[f].apply(lambda x: conv_values(x, f, list_c))\n",
        "    df[\"feat\"] = df.apply(lambda x: list(x[1:].values), axis=1)\n",
        "    df[\"feat\"] = df[\"feat\"].apply(flatten_list)\n",
        "    df = df[[\"id\", \"feat\"]]\n",
        "    df = df.explode(\"feat\").reset_index(drop=True)\n",
        "    df = df.merge(seed_count, on=\"feat\", how=\"left\")\n",
        "    df = df[[\"id\", \"imp\"]]\n",
        "    df = df.groupby(\"id\")[\"imp\"].sum().reset_index()\n",
        "    df.columns = [\"id\", \"score\"]\n",
        "    data = data.merge(df, on=\"id\", how=\"left\")\n",
        "    return data\n",
        "\n",
        "def get_extn(data, seed_ids, label, extn_path, x=2):\n",
        "    \"\"\"\n",
        "    Retrieve a set of users from the neighbor set based on their score and save them to a file.\n",
        "\n",
        "    :param data: DataFrame containing user data along with the scores\n",
        "    :param seed_ids: List of customer IDs that are in the seed set\n",
        "    :param label: Label column indicating whether the user clicked the ad\n",
        "    :param extn_path: Path to store the extended user set\n",
        "    :param x: Scale of extension needed\n",
        "    :return: Click rate of the extended user set\n",
        "    \"\"\"\n",
        "    # Drop users who don't have a score (those are not neighbors)\n",
        "    data = data.dropna(subset=[\"score\"])\n",
        "    # Sort the users by score in descending order\n",
        "    data = data.sort_values(by=\"score\", ascending=False)\n",
        "    # Select the top users\n",
        "    extn = data.iloc[:x * len(seed_ids), :][[\"CustomerID\"]]\n",
        "    # Write the extended user IDs to a file\n",
        "    extn.to_csv(extn_path, index=False)\n",
        "    # Calculate the click rate of the extended user set\n",
        "    extn_click_rate = data.iloc[:x * len(seed_ids), :][label].mean()\n",
        "    extn_click_rate = round(extn_click_rate * 100, 2)\n",
        "    return extn_click_rate\n"
      ],
      "metadata": {
        "id": "Ms2R8KEki2Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSH graph model"
      ],
      "metadata": {
        "id": "3Vg6axw371YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasketch import MinHash, MinHashLSHForest\n",
        "\n",
        "class LSHGraph:\n",
        "    \"\"\"\n",
        "    Locality-Sensitive Hashing (LSH) Graph Model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, model, features, id_col=\"id\", n_perm=10):\n",
        "        \"\"\"\n",
        "        Initialize the LSHGraph.\n",
        "\n",
        "        :param df: DataFrame containing user features\n",
        "        :param model: MinHashLSHForest model for LSH operations\n",
        "        :param features: List of features in the dataset\n",
        "        :param id_col: Column name for user IDs\n",
        "        :param n_perm: Number of permutations for the LSH model\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.model = model\n",
        "        self.features = features\n",
        "        self.id_col = id_col\n",
        "        self.n_perm = n_perm\n",
        "\n",
        "    def update_graph(self):\n",
        "        \"\"\"\n",
        "        Update the LSH graph by adding MinHash values for each user in the DataFrame.\n",
        "        \"\"\"\n",
        "        for i, row in self.df[self.features].iterrows():\n",
        "            if i % 5000 == 0:\n",
        "                print(f\"Processing {i} of {self.df.shape[0]}\")\n",
        "            m = MinHash(num_perm=self.n_perm)\n",
        "            m = self.get_hash(m, row)\n",
        "            self.model.add(self.df[self.id_col][i], m)\n",
        "        self.model.index()\n",
        "\n",
        "    def extract_neighbors(self, seed, k=3):\n",
        "     \"\"\"\n",
        "    Retrieve neighbors of seed set users from the LSH graph along with their similarity scores.\n",
        "\n",
        "    :param seed: List of customer IDs from the seed set\n",
        "    :param k: Number of neighbors to retrieve for each seed set user\n",
        "    :return: Dictionary where keys are seed_ids and values are lists of tuples (neighbor_id, similarity_score)\n",
        "     \"\"\"\n",
        "     neighbors_with_scores_dict = {}\n",
        "\n",
        "    # Filter the seed set from the dataframe\n",
        "     seed_df = self.df[self.df[self.id_col].isin(seed)]\n",
        "\n",
        "     for i, row in seed_df[self.features].iterrows():\n",
        "        # Initialize a MinHash object for the current row\n",
        "        m = MinHash(num_perm=self.n_perm)\n",
        "        m = self.get_hash(m, row)\n",
        "\n",
        "        # Query for similar neighbors from the model\n",
        "        similar_neighbors = self.model.query(m, k)\n",
        "\n",
        "        # List to hold the neighbors with their similarity scores\n",
        "        neighbor_scores = []\n",
        "\n",
        "        # Loop over each neighbor found and compute similarity score\n",
        "        for neighbor_id in similar_neighbors:\n",
        "            # Recompute the MinHash for the neighbor directly here, since MinHash is not stored\n",
        "            neighbor_row = self.df[self.df[self.id_col] == neighbor_id].iloc[0]\n",
        "            neighbor_m = MinHash(num_perm=self.n_perm)\n",
        "            neighbor_m = self.get_hash(neighbor_m, neighbor_row[self.features])\n",
        "\n",
        "            # Calculate Jaccard similarity between the seed and the neighbor\n",
        "            similarity_score = m.jaccard(neighbor_m)  # Jaccard similarity score\n",
        "            neighbor_scores.append((neighbor_id, similarity_score))\n",
        "\n",
        "        # Sort by similarity score and select the top k neighbors\n",
        "        neighbor_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Store the top k neighbors and their similarity scores in the dictionary\n",
        "        neighbors_with_scores_dict[self.df[self.id_col][i]] = neighbor_scores[:k]\n",
        "\n",
        "     return neighbors_with_scores_dict\n",
        "\n",
        "\n",
        "\n",
        "    def get_hash(self, m, row):\n",
        "        \"\"\"\n",
        "        Encode a user's features using MinHash.\n",
        "\n",
        "        :param m: MinHash object to update\n",
        "        :param row: User's feature list\n",
        "        :return: Updated MinHash object\n",
        "        \"\"\"\n",
        "        for d in row:\n",
        "            if type(d) == list:\n",
        "                for e in d:\n",
        "                    m.update(str(e).encode('utf-8'))\n",
        "            else:\n",
        "                m.update(str(d).encode('utf-8'))\n",
        "        return m\n"
      ],
      "metadata": {
        "id": "Uq6nAl2oicPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SjuJQPSxkZ1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing LSH graph"
      ],
      "metadata": {
        "id": "eGuaDv3s76ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from yaml import CLoader as Loader, load\n",
        "from datasketch import MinHash, MinHashLSHForest\n",
        "\n",
        "with open(\"config.yaml\") as stream:\n",
        "    config = load(stream, Loader=Loader)\n",
        "\n",
        "columns_list = [\n",
        "\n",
        "    'ProductID',\n",
        "    'TransactionDate',\n",
        "    'Quantity',\n",
        "    'TotalValue',\n",
        "    'Price',\n",
        "    'ProductName',\n",
        "    'Category'\n",
        "]\n",
        "label = \"TotalValue\"\n",
        "n_perm = 100\n",
        "id_col = \"CustomerID\"\n",
        "features = [ 'CustomerName', 'Region', 'SignupDate'\n",
        "     , 'TransactionDate', 'Quantity', 'Price',\n",
        "       'ProductName', 'Category']\n",
        "\n",
        "data_path = \"/content/merged_dataset.csv\"\n",
        "seed_path = \"/content/data/seed.csv\"\n",
        "extn_path = \"content/data/extn.csv\"\n",
        "list_cols = columns_list\n",
        "thresh = 16\n",
        "model_path = \"/content/data/lshgraph\"\n",
        "count_path = \"/content/data/count_df.csv\"\n",
        "clean_data_path = \"/content/data/processed.json\"\n",
        "\n",
        "\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "\n",
        "data.to_json(clean_data_path)\n",
        "\n",
        "\n",
        "count_df = count_fn(data, features, list_cols)\n",
        "\n",
        "count_df.to_csv(count_path, index=False)\n",
        "\n",
        "data = pd.read_json(clean_data_path)\n",
        "\n",
        "\n",
        "df = data.drop(label, axis=1)\n",
        "\n",
        "\n",
        "lsh = MinHashLSHForest(num_perm=n_perm)\n",
        "\n",
        "lsh_graph = LSHGraph(df, lsh, features, id_col=id_col, n_perm=n_perm)\n",
        "\n",
        "lsh_graph.update_graph()\n",
        "\n",
        "with open(model_path, \"wb\") as f:\n",
        "    pickle.dump(lsh_graph, f)\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_json(clean_data_path)\n",
        "\n",
        "seed = pd.read_csv(seed_path)\n",
        "\n",
        "seed_ids = list(seed[\"CustomerID\"])\n",
        "\n",
        "\n",
        "lsh_graph = pickle.load(open(model_path, \"rb\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnaGUp3NigjL",
        "outputId": "dcf4cb7d-5e6c-465f-b2ae-ad1d77e94329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e45637b3d2d3>:55: FutureWarning: The provided callable <built-in function sum> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
            "  count_df[\"sum\"] = count_df.groupby(\"feature\")[\"count\"].transform(sum)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 0 of 199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "neighbors = lsh_graph.extract_neighbors(seed_ids,k=4)\n"
      ],
      "metadata": {
        "id": "X_B9QNZzi8Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_and_sort_neighbors(neighbors_dict):\n",
        "\n",
        "    filtered_dict = {}\n",
        "\n",
        "    for customer_id, neighbors in neighbors_dict.items():\n",
        "        # Remove self-referencing neighbors\n",
        "        filtered_neighbors = [(neighbor_id, score) for neighbor_id, score in neighbors if neighbor_id != customer_id]\n",
        "\n",
        "        # Sort neighbors by similarity score in descending order\n",
        "        filtered_neighbors.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Only keep entries with 3 or more neighbors\n",
        "        filtered_dict[customer_id] = filtered_neighbors[:3]\n",
        "\n",
        "    return filtered_dict\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE2TMQ2UkxGI",
        "outputId": "afdb11da-c60b-4111-ba0c-3dfc75619b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C0001': [], 'C0002': [('C0054', 0.09), ('C0143', 0.08), ('C0022', 0.06)], 'C0003': [], 'C0004': [], 'C0005': [('C0054', 0.09), ('C0143', 0.08), ('C0022', 0.07)], 'C0006': [('C0191', 0.06), ('C0190', 0.05), ('C0126', 0.05)], 'C0007': [('C0053', 0.1), ('C0186', 0.03), ('C0146', 0.03)], 'C0008': [('C0116', 0.07), ('C0157', 0.06), ('C0020', 0.05)], 'C0009': [('C0057', 0.08), ('C0019', 0.05), ('C0081', 0.05)], 'C0010': [('C0057', 0.05), ('C0081', 0.05), ('C0019', 0.03)], 'C0011': [('C0031', 0.06), ('C0052', 0.05), ('C0181', 0.05)], 'C0012': [], 'C0013': [], 'C0014': [('C0060', 0.25), ('C0151', 0.11), ('C0057', 0.08)], 'C0015': [('C0123', 0.11)], 'C0016': [], 'C0017': [('C0075', 0.06), ('C0164', 0.06), ('C0019', 0.05)], 'C0018': [('C0110', 0.08), ('C0098', 0.05), ('C0033', 0.05)], 'C0019': [('C0075', 0.07), ('C0164', 0.07), ('C0017', 0.05)], 'C0020': [('C0157', 0.06), ('C0116', 0.05), ('C0189', 0.04)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LDk5dVyy5PWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_neighbors_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrWOzA9ek41n",
        "outputId": "422b243a-1d84-4d77-ea4f-b587cfd58af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOufzMui7DD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}